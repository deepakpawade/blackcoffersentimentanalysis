{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notebook for code playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_excel('./Input.xlsx')\n",
    "\n",
    "df = copy.deepcopy(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# define a function to extract the content of a link\n",
    "def extract_content(url):\n",
    "    # send an HTTP GET request to the website\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # parse the HTML content of the website using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # if the page returns 404\n",
    "    if soup.find('div', class_='td-404-title'):\n",
    "        print(f'{url} 404 Error: Page not found')\n",
    "        return None\n",
    "\n",
    "    # extract the content of the article\n",
    "    content = \"\"\n",
    "    try:\n",
    "        for p in soup.find(\"div\", {\"class\": \"td-post-content\"}).find_all(\"p\"):\n",
    "            # content += p.text.strip() + \"\\n\"\n",
    "            content += p.text.strip() + \" \"\n",
    "    except Exception as e:\n",
    "        print(f'content : {e}: {url}')\n",
    "    return content\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column to the dataframe to store the content\n",
    "df['content'] = ''\n",
    "\n",
    "# iterate over the rows of the dataframe and extract the content of each link\n",
    "for i, row in df.iterrows():\n",
    "    try:\n",
    "        link = row['URL']\n",
    "        content = extract_content(link)\n",
    "        df.at[i, 'content'] = content\n",
    "    except Exception as e:\n",
    "         print(f'content : {e}: {link}')\n",
    "# print the resulting dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./content.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "URL_ID        0\n",
       "URL           0\n",
       "content       5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['content'] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "URL_ID        0\n",
       "URL           0\n",
       "content       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_dir = 'StopWords/'\n",
    "def get_stopwords():\n",
    "    \"\"\"\n",
    "    Get all the stopwords from the StopWords folder\n",
    "    \"\"\"\n",
    "    stopwords_files = ['StopWords_Auditor.txt',\n",
    "                       'StopWords_Currencies.txt',\n",
    "                       'StopWords_DatesandNumbers.txt',\n",
    "                       'StopWords_Generic.txt',\n",
    "                       'StopWords_GenericLong.txt',\n",
    "                       'StopWords_Geographic.txt',\n",
    "                       'StopWords_Names.txt']\n",
    "                       \n",
    "    stopwords = set()\n",
    "    for filename in stopwords_files:\n",
    "        with open(stopwords_dir + filename, 'r', encoding='latin-1', errors='replace') as file:\n",
    "            stopwords.update([word.strip() for word in file])\n",
    "    # print(stopwords)\n",
    "    return stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positive_words(stop_words):\n",
    "    \"\"\"\n",
    "    Get all the positive words from the MasterDictionary folder\n",
    "    \"\"\"\n",
    "    with open('MasterDictionary/positive-words.txt', 'r') as file:\n",
    "        positive_words = [word.strip() for word in file.readlines()]\n",
    "\n",
    "    # Remove stop words from the dictionaries\n",
    "    pos_dict = {word for word in positive_words if word not in stop_words}\n",
    "    return set(pos_dict)\n",
    "\n",
    "def get_negative_words(stop_words):\n",
    "    \"\"\"\n",
    "    Get all the negative words from the MasterDictionary folder\n",
    "    \"\"\"\n",
    "    with open('MasterDictionary/negative-words.txt', 'r') as file:\n",
    "        negative_words = [word.strip() for word in file.readlines()]\n",
    "\n",
    "    neg_dict = {word for word in negative_words if word not in stop_words}\n",
    "    return set(neg_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment_scores(text, pos_dict, neg_dict):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Calculate positive and negative scores\n",
    "    pos_score = sum(1 for word in tokens if word in pos_dict)\n",
    "    neg_score = sum(-1 for word in tokens if word in neg_dict) * -1\n",
    "    \n",
    "    # Calculate polarity score\n",
    "    polarity_score = (pos_score - neg_score) / (pos_score + neg_score + 0.000001)\n",
    "    \n",
    "    # Calculate subjectivity score\n",
    "    subjectivity_score = (pos_score + abs(neg_score)) / (len(tokens) + 0.000001)\n",
    "    \n",
    "    return pos_score, neg_score, polarity_score, subjectivity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    This function cleans the given text by removing punctuations and stop words.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_text = \"\"\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and word not in string.punctuation:\n",
    "            cleaned_text += word + \" \"\n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllable_count(words):\n",
    "    vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word[-2:] == 'es' or word[-2:] == 'ed':\n",
    "            continue\n",
    "        word_count = 0\n",
    "        for char in word:\n",
    "            if char.lower() in vowels:\n",
    "                word_count += 1\n",
    "            count += word_count\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_length(words):\n",
    "    total_char = 0\n",
    "    for word in words:\n",
    "        total_char += len(word)\n",
    "    return total_char/len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def personal_pronouns(text):\n",
    "    pattern = r'\\b(I|we|my|ours|us)\\b(?!\\b[A-Z]{2}\\b)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['POSITIVE SCORE'] = 0\n",
    "df['NEGATIVE SCORE'] = 0\n",
    "df['POLARITY SCORE'] = 0\n",
    "df['SUBJECTIVITY SCORE'] = 0\n",
    "df['AVG SENTENCE LENGTH'] = 0\n",
    "df['PERCENTAGE OF COMPLEX WORDS'] = 0\n",
    "df['FOG INDEX'] = 0\n",
    "df['AVG NUMBER OF WORDS PER SENTENCE'] = 0\n",
    "df['COMPLEX WORD COUNT'] = 0\n",
    "df['WORD COUNT'] = 0\n",
    "df['SYLLABLE PER WORD'] = 0\n",
    "df['PERSONAL PRONOUNS'] = 0\n",
    "df['AVG WORD LENGTH'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = get_stopwords()\n",
    "pos_dict = get_positive_words(stop_words=stop_words)\n",
    "neg_dict = get_negative_words(stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    text = row['content']\n",
    "    sentences_uncleaned = nltk.sent_tokenize(text)\n",
    "    words_uncleaned = nltk.word_tokenize(text)\n",
    "    cleaned_text = clean_text(text)\n",
    "    sentences_cleaned = nltk.sent_tokenize(cleaned_text)\n",
    "    words_cleaned = nltk.word_tokenize(cleaned_text)\n",
    "    pos,neg, pol, sub = calculate_sentiment_scores(text,pos_dict,neg_dict)\n",
    "    avg_sent_len = len(words_cleaned)/len(sentences_uncleaned)\n",
    "    comp_word_count = sum([1 for word in words_cleaned if len(word) > 2])\n",
    "    per_comp_words = comp_word_count/len(words_cleaned)\n",
    "    fog = 0.4 * (avg_sent_len + per_comp_words)\n",
    "    avg_words_per_sent = len(words_cleaned)/len(sentences_uncleaned)\n",
    "    syl_per_word = syllable_count(words_cleaned)\n",
    "    per_pro = personal_pronouns(text)\n",
    "    avg_word_len = average_word_length(words_cleaned)\n",
    "\n",
    "    df.at[i,'POSITIVE SCORE'] = pos\n",
    "    df.at[i,'NEGATIVE SCORE'] = neg\n",
    "    df.at[i,'POLARITY SCORE'] = pol\n",
    "    df.at[i,'SUBJECTIVITY SCORE'] = sub\n",
    "    df.at[i,'AVG SENTENCE LENGTH'] = avg_sent_len\n",
    "    df.at[i,'PERCENTAGE OF COMPLEX WORDS'] = per_comp_words\n",
    "    df.at[i,'FOG INDEX'] = fog\n",
    "    df.at[i,'AVG NUMBER OF WORDS PER SENTENCE'] = avg_words_per_sent\n",
    "    df.at[i,'WORD COUNT'] = len(words_cleaned)\n",
    "    df.at[i,'SYLLABLE PER WORD'] = syl_per_word\n",
    "    df.at[i,'PERSONAL PRONOUNS'] = per_pro\n",
    "    df.at[i,'AVG WORD LENGTH'] = avg_word_len\n",
    "    df.at[i,'COMPLEX WORD COUNT'] = comp_word_count\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0','content'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>66</td>\n",
       "      <td>34</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.050226</td>\n",
       "      <td>15.333333</td>\n",
       "      <td>0.951304</td>\n",
       "      <td>6.513855</td>\n",
       "      <td>15.333333</td>\n",
       "      <td>1094</td>\n",
       "      <td>1150</td>\n",
       "      <td>12669</td>\n",
       "      <td>1</td>\n",
       "      <td>7.137391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>60</td>\n",
       "      <td>38</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>0.060197</td>\n",
       "      <td>9.734177</td>\n",
       "      <td>0.932380</td>\n",
       "      <td>4.266623</td>\n",
       "      <td>9.734177</td>\n",
       "      <td>717</td>\n",
       "      <td>769</td>\n",
       "      <td>6658</td>\n",
       "      <td>6</td>\n",
       "      <td>6.356307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>65</td>\n",
       "      <td>38</td>\n",
       "      <td>0.262136</td>\n",
       "      <td>0.054239</td>\n",
       "      <td>11.857143</td>\n",
       "      <td>0.968876</td>\n",
       "      <td>5.130407</td>\n",
       "      <td>11.857143</td>\n",
       "      <td>965</td>\n",
       "      <td>996</td>\n",
       "      <td>10726</td>\n",
       "      <td>2</td>\n",
       "      <td>7.183735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>58</td>\n",
       "      <td>23</td>\n",
       "      <td>0.432099</td>\n",
       "      <td>0.042100</td>\n",
       "      <td>12.973684</td>\n",
       "      <td>0.943205</td>\n",
       "      <td>5.566756</td>\n",
       "      <td>12.973684</td>\n",
       "      <td>930</td>\n",
       "      <td>986</td>\n",
       "      <td>9473</td>\n",
       "      <td>12</td>\n",
       "      <td>6.584178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>https://insights.blackcoffer.com/man-and-machi...</td>\n",
       "      <td>44</td>\n",
       "      <td>23</td>\n",
       "      <td>0.313433</td>\n",
       "      <td>0.047518</td>\n",
       "      <td>12.327273</td>\n",
       "      <td>0.942478</td>\n",
       "      <td>5.307900</td>\n",
       "      <td>12.327273</td>\n",
       "      <td>639</td>\n",
       "      <td>678</td>\n",
       "      <td>6435</td>\n",
       "      <td>18</td>\n",
       "      <td>6.560472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>145</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-in...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028025</td>\n",
       "      <td>11.878788</td>\n",
       "      <td>0.987245</td>\n",
       "      <td>5.146413</td>\n",
       "      <td>11.878788</td>\n",
       "      <td>387</td>\n",
       "      <td>392</td>\n",
       "      <td>3903</td>\n",
       "      <td>2</td>\n",
       "      <td>7.104592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>146</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "      <td>22</td>\n",
       "      <td>27</td>\n",
       "      <td>-0.102041</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>11.062500</td>\n",
       "      <td>0.979284</td>\n",
       "      <td>4.816714</td>\n",
       "      <td>11.062500</td>\n",
       "      <td>520</td>\n",
       "      <td>531</td>\n",
       "      <td>5426</td>\n",
       "      <td>9</td>\n",
       "      <td>7.143126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>147</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "      <td>36</td>\n",
       "      <td>12</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.039344</td>\n",
       "      <td>13.285714</td>\n",
       "      <td>0.961598</td>\n",
       "      <td>5.698925</td>\n",
       "      <td>13.285714</td>\n",
       "      <td>626</td>\n",
       "      <td>651</td>\n",
       "      <td>6460</td>\n",
       "      <td>1</td>\n",
       "      <td>6.826421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>148</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "      <td>26</td>\n",
       "      <td>46</td>\n",
       "      <td>-0.277778</td>\n",
       "      <td>0.057371</td>\n",
       "      <td>10.338462</td>\n",
       "      <td>0.973214</td>\n",
       "      <td>4.524670</td>\n",
       "      <td>10.338462</td>\n",
       "      <td>654</td>\n",
       "      <td>672</td>\n",
       "      <td>6779</td>\n",
       "      <td>2</td>\n",
       "      <td>6.678571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>150</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "      <td>33</td>\n",
       "      <td>41</td>\n",
       "      <td>-0.108108</td>\n",
       "      <td>0.065429</td>\n",
       "      <td>8.953846</td>\n",
       "      <td>0.991409</td>\n",
       "      <td>3.978102</td>\n",
       "      <td>8.953846</td>\n",
       "      <td>577</td>\n",
       "      <td>582</td>\n",
       "      <td>5592</td>\n",
       "      <td>8</td>\n",
       "      <td>6.792096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL  \\\n",
       "0        37  https://insights.blackcoffer.com/ai-in-healthc...   \n",
       "1        38  https://insights.blackcoffer.com/what-if-the-c...   \n",
       "2        39  https://insights.blackcoffer.com/what-jobs-wil...   \n",
       "3        41  https://insights.blackcoffer.com/will-ai-repla...   \n",
       "4        42  https://insights.blackcoffer.com/man-and-machi...   \n",
       "..      ...                                                ...   \n",
       "104     145  https://insights.blackcoffer.com/blockchain-in...   \n",
       "105     146  https://insights.blackcoffer.com/blockchain-fo...   \n",
       "106     147  https://insights.blackcoffer.com/the-future-of...   \n",
       "107     148  https://insights.blackcoffer.com/big-data-anal...   \n",
       "108     150  https://insights.blackcoffer.com/challenges-an...   \n",
       "\n",
       "     POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0                66              34        0.320000            0.050226   \n",
       "1                60              38        0.224490            0.060197   \n",
       "2                65              38        0.262136            0.054239   \n",
       "3                58              23        0.432099            0.042100   \n",
       "4                44              23        0.313433            0.047518   \n",
       "..              ...             ...             ...                 ...   \n",
       "104              11              11        0.000000            0.028025   \n",
       "105              22              27       -0.102041            0.050000   \n",
       "106              36              12        0.500000            0.039344   \n",
       "107              26              46       -0.277778            0.057371   \n",
       "108              33              41       -0.108108            0.065429   \n",
       "\n",
       "     AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0              15.333333                     0.951304   6.513855   \n",
       "1               9.734177                     0.932380   4.266623   \n",
       "2              11.857143                     0.968876   5.130407   \n",
       "3              12.973684                     0.943205   5.566756   \n",
       "4              12.327273                     0.942478   5.307900   \n",
       "..                   ...                          ...        ...   \n",
       "104            11.878788                     0.987245   5.146413   \n",
       "105            11.062500                     0.979284   4.816714   \n",
       "106            13.285714                     0.961598   5.698925   \n",
       "107            10.338462                     0.973214   4.524670   \n",
       "108             8.953846                     0.991409   3.978102   \n",
       "\n",
       "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                           15.333333                1094        1150   \n",
       "1                            9.734177                 717         769   \n",
       "2                           11.857143                 965         996   \n",
       "3                           12.973684                 930         986   \n",
       "4                           12.327273                 639         678   \n",
       "..                                ...                 ...         ...   \n",
       "104                         11.878788                 387         392   \n",
       "105                         11.062500                 520         531   \n",
       "106                         13.285714                 626         651   \n",
       "107                         10.338462                 654         672   \n",
       "108                          8.953846                 577         582   \n",
       "\n",
       "     SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0                12669                  1         7.137391  \n",
       "1                 6658                  6         6.356307  \n",
       "2                10726                  2         7.183735  \n",
       "3                 9473                 12         6.584178  \n",
       "4                 6435                 18         6.560472  \n",
       "..                 ...                ...              ...  \n",
       "104               3903                  2         7.104592  \n",
       "105               5426                  9         7.143126  \n",
       "106               6460                  1         6.826421  \n",
       "107               6779                  2         6.678571  \n",
       "108               5592                  8         6.792096  \n",
       "\n",
       "[109 rows x 15 columns]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('./output.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0398511a9cde84ab83a2fa188ff6508a33d7c0397b3581839dbbf3238a247df4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
